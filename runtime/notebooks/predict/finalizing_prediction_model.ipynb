{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bff6e38",
   "metadata": {},
   "source": [
    "# Finalizing a Prediction Strategy\n",
    "\n",
    "In previous notebooks, we explored different data preprocessing techniques, feature engineering, and testing various sklearn models for predicting NBA games.\n",
    "\n",
    "Now, we will attempt to finalize a prediction strategy based on what we've learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb03a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1b628f",
   "metadata": {},
   "source": [
    "## Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088f8bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"../../data/processed/processed_team_dataset.csv\"\n",
    "\n",
    "df = pd.read_csv(path_to_data, index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d154a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we have no null values\n",
    "\n",
    "# Finding the number of missing values in each column\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Printing the number of missing values in each column\n",
    "print(sum(missing_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e860ee",
   "metadata": {},
   "source": [
    "## Separate relevant features from the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af80269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop these columns (most of the are non_numeric or are not useful for machine learning)\n",
    "drop_columns = [\"date\", \"season\", \"team\", \"team_opp\", \"won\"]\n",
    "\n",
    "selected_columns = df.columns[~df.columns.isin(drop_columns)]\n",
    "\n",
    "# Selected features\n",
    "features_df = df[selected_columns]\n",
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa3e250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label we want to predict\n",
    "label = df[\"won\"]\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666b9bb9",
   "metadata": {},
   "source": [
    "### Remember that elo is about 64.48% accurate at predicting winner of a game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20bed72",
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_point_spread = df[\"team_point_diff_proj\"]\n",
    "projected_win_from_elo = projected_point_spread > 0\n",
    "\n",
    "print(metrics.accuracy_score(label, projected_win_from_elo))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9bc4b8",
   "metadata": {},
   "source": [
    "## Scale data for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f1645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaled_features_df = pd.DataFrame(scaler.fit_transform(features_df), columns=features_df.columns)\n",
    "scaled_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4850a0f2",
   "metadata": {},
   "source": [
    "## Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c8c1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_ratio = 0.3\n",
    "x_train_scaled, x_test_scaled, y_train_scaled, y_test_scaled = train_test_split(scaled_features_df, label, test_size = 0.3) # 70% data is training and 30% is for testing\n",
    "\n",
    "x_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c9e796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "# Logistic Regression with max_iter=200 \n",
    "log_model = LogisticRegression(max_iter=200, verbose=2, random_state=42)\n",
    "log_model.fit(x_train_scaled, y_train_scaled)\n",
    "y_pred_log = log_model.predict(x_test_scaled)\n",
    "print(metrics.accuracy_score(y_test_scaled, y_pred_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451a0408",
   "metadata": {},
   "source": [
    "From our previous tests, we found that logistic regression gave around 65% accuracy. Let us try to find the optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7c18a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# param_grid = {\n",
    "#     'C': [0.001, 0.01, 0.1, 0.5, 0.75, 1, 1.25, 1.5, 2, 5, 10, 100],\n",
    "#     'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "#     'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "# }\n",
    "\n",
    "# log_model = LogisticRegression(max_iter=1000)\n",
    "# clf = GridSearchCV(log_model, param_grid, cv=5, scoring='accuracy', verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1edbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.fit(x_train_scaled, y_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915636f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Best Parameters:\", clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1742d76",
   "metadata": {},
   "source": [
    "### Train using best parameters\n",
    "\n",
    "We have found \"optimal\" parameters with GridSearchCV:\n",
    "\n",
    "Best Parameters: {'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'}\n",
    "\n",
    "Let's evaluate the model using these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac4d6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_log_model = LogisticRegression(max_iter=1000, verbose=2, random_state=42, C=0.01, penalty='l2', solver='liblinear')\n",
    "opt_log_model.fit(x_train_scaled, y_train_scaled)\n",
    "y_pred = opt_log_model.predict(x_test_scaled)\n",
    "print(metrics.accuracy_score(y_test_scaled, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734ff23b",
   "metadata": {},
   "source": [
    "It seems like the parameters do not give much of a performance bump. Nonetheless, let's use this \"optimized\" model further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3cb526",
   "metadata": {},
   "source": [
    "### See accuracy of \"high\" confidence predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a4c8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with a probability for each class\n",
    "y_pred_prob_log = opt_log_model.predict_proba(x_test_scaled)\n",
    "y_pred_prob_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aa6776",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_threshold = 0.7\n",
    "\n",
    "# Identify predictions where the probability of either class 0 or class 1 meets or exceeds the threshold\n",
    "high_confidence_indices = np.where((y_pred_prob_log[:, 0] >= prob_threshold) | (y_pred_prob_log[:, 1] >= prob_threshold))\n",
    "\n",
    "high_confidence_indices = list(high_confidence_indices[0])\n",
    "print(len(high_confidence_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6047339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the predictions and actual labels for these high confidence predictions\n",
    "high_conf_predictions = np.argmax(y_pred_prob_log[high_confidence_indices], axis=1)\n",
    "high_conf_actual = y_test_scaled.reset_index(drop=True)\n",
    "high_conf_actual = high_conf_actual[high_confidence_indices]\n",
    "high_conf_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00446f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.accuracy_score(high_conf_actual, high_conf_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ce6187",
   "metadata": {},
   "source": [
    "### See accuracy of \"low\" confidence predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf82626",
   "metadata": {},
   "outputs": [],
   "source": [
    "## What about the accuracy of those outside of the \"high_conf_predictions\"?\n",
    "\n",
    "low_confidence_indices = [i for i in range(len(y_pred_prob_log)) if i not in high_confidence_indices]\n",
    "print(len(low_confidence_indices))\n",
    "assert len(low_confidence_indices) + len(high_confidence_indices) == len(y_pred_prob_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8420d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the predictions and actual labels for these \"low\" confidence predictions\n",
    "low_conf_predictions = np.argmax(y_pred_prob_log[low_confidence_indices], axis=1)\n",
    "low_conf_actual = y_test_scaled.reset_index(drop=True)\n",
    "low_conf_actual = low_conf_actual[low_confidence_indices]\n",
    "low_conf_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b8195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.accuracy_score(low_conf_actual, low_conf_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4fe224",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_low_confidence = x_test_scaled.reset_index(drop=True)\n",
    "x_test_low_confidence = x_test_low_confidence.loc[low_confidence_indices]\n",
    "assert(x_test_low_confidence.index.values.tolist() == low_conf_actual.index.values.tolist())\n",
    "x_test_low_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1bffa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "elo_proj_low_confidence = x_test_low_confidence[\"team_point_diff_proj\"] > 0\n",
    "elo_proj_low_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2006edc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.accuracy_score(low_conf_actual, elo_proj_low_confidence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b33de1",
   "metadata": {},
   "source": [
    "## K-fold Cross Validation\n",
    "\n",
    "Let's see if our log model is good for future data. What we can try to do is a [k-fold cross validation](https://www.youtube.com/watch?v=kituDjzXwfE&t=698s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a61d560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set X and Y as the data and labels\n",
    "X = scaled_features_df\n",
    "Y = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65530d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the model again\n",
    "opt_log_model_cv = LogisticRegression(max_iter=1000, verbose=2, random_state=42, C=0.01, penalty='l2', solver='liblinear')\n",
    "log_model = LogisticRegression(max_iter=200, verbose=2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e1bc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=20, shuffle=True, random_state=42)\n",
    "\n",
    "# Performing k-fold cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(opt_log_model, X, Y, cv=kf, scoring=\"accuracy\")\n",
    "\n",
    "# Prining CV scores\n",
    "print(\"CV scores:\", cv_scores)\n",
    "\n",
    "# Averaging CV scores\n",
    "import numpy as np\n",
    "print(\"Average score:\", np.round(cv_scores.mean(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad88c6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we did just Elo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6e0cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3c5e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "elo_features = [\"team_elo_before\", \"team_opp_elo_before\", \"team_expected_win_probability\", \"team_point_diff_proj\"]\n",
    "non_elo_features = scaled_features_df.columns[~scaled_features_df.columns.isin(elo_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12607036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set X and Y as the data and labels\n",
    "X = scaled_features_df[non_elo_features]\n",
    "Y = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8494e7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6cf714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the model again\n",
    "opt_log_model_cv = LogisticRegression(max_iter=1000, verbose=2, random_state=42, C=0.01, penalty='l2', solver='liblinear')\n",
    "log_model = LogisticRegression(max_iter=200, verbose=2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b689f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=20, shuffle=True, random_state=42)\n",
    "\n",
    "# Performing k-fold cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(opt_log_model, X, Y, cv=kf, scoring=\"accuracy\")\n",
    "\n",
    "# Prining CV scores\n",
    "print(\"CV scores:\", cv_scores)\n",
    "\n",
    "# Averaging CV scores\n",
    "import numpy as np\n",
    "print(\"Average score:\", np.round(cv_scores.mean(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587622a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ed43ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b256f981",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
